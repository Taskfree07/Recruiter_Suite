"""
Enhanced ML-Powered Salary Prediction Service

This service uses advanced machine learning with feature engineering to predict salaries
with high accuracy. Uses multiple features including job title, location, experience,
company characteristics, and required skills.

Key Improvements:
1. Uses XGBoost for better accuracy
2. Feature engineering: company age, location cost of living, skill combinations
3. Robust preprocessing with outlier handling
4. Cross-validation for model evaluation
5. Feature importance analysis
"""

import json
import os
import sys
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import joblib
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

# Redirect print statements to stderr when in API mode
def log(message):
    """Print to stderr to avoid polluting JSON output"""
    if len(sys.argv) > 1:
        print(message, file=sys.stderr)
    else:
        print(message)


class EnhancedSalaryPredictor:
    def __init__(self, dataset_path: str = None):
        """
        Initialize Enhanced Salary Predictor

        Args:
            dataset_path: Path to salary dataset CSV file
        """
        self.dataset_path = dataset_path or os.path.join(
            os.path.dirname(__file__), 'data', 'salary_data_cleaned.csv'
        )
        self.model_path = os.path.join(
            os.path.dirname(__file__), 'models', 'salary_model_enhanced.pkl'
        )
        self.encoders_path = os.path.join(
            os.path.dirname(__file__), 'models', 'salary_encoders_enhanced.pkl'
        )

        self.model = None
        self.encoders = {}
        self.scaler = StandardScaler()
        self.feature_columns = []
        self.dataset_loaded = False
        self.df = None

        # Enhanced Cost of Living Index (more comprehensive)
        self.col_index = {
            'san francisco': 1.62, 'new york': 1.68, 'seattle': 1.41,
            'austin': 1.06, 'boston': 1.47, 'los angeles': 1.41,
            'chicago': 1.08, 'denver': 1.12, 'atlanta': 0.96,
            'dallas': 0.96, 'miami': 1.10, 'phoenix': 0.96,
            'philadelphia': 1.06, 'san diego': 1.44, 'portland': 1.26,
            'washington': 1.52, 'baltimore': 1.09, 'columbus': 0.91,
            'charlotte': 0.94, 'raleigh': 0.95, 'nashville': 0.96,
            # State-level fallbacks
            'california': 1.35, 'new york': 1.30, 'washington': 1.20,
            'massachusetts': 1.25, 'colorado': 1.10, 'oregon': 1.15,
            'texas': 0.95, 'florida': 0.98, 'georgia': 0.92,
            'north carolina': 0.93, 'tennessee': 0.90, 'ohio': 0.88
        }

        # Skill value multipliers (how much each skill increases salary)
        self.skill_multipliers = {
            'python': 1.15,
            'R': 1.10,
            'spark': 1.20,
            'aws': 1.18,
            'excel': 1.05,
            'machine_learning': 1.22,
            'deep_learning': 1.25,
            'sql': 1.08,
            'tableau': 1.07
        }

        # Try to load existing model or train new one
        self._initialize()

    def _initialize(self):
        """Initialize: load existing model or train new one"""
        if os.path.exists(self.model_path) and os.path.exists(self.encoders_path):
            log("Loading pre-trained enhanced model...")
            self._load_model()
        elif os.path.exists(self.dataset_path):
            log("Training new enhanced model from dataset...")
            self.load_dataset()
            self.train_model()
        else:
            log(f"Dataset not found at: {self.dataset_path}")
            log("Using fallback prediction mode")

    def load_dataset(self) -> bool:
        """Load and preprocess salary dataset"""
        try:
            log(f"Loading dataset from: {self.dataset_path}")
            self.df = pd.read_csv(self.dataset_path)

            log(f"Dataset loaded: {len(self.df)} rows")

            # Standardize column names
            self.df.columns = [col.strip().replace(' ', '_').lower() for col in self.df.columns]

            # Drop rows with missing salary
            self.df = self.df.dropna(subset=['avg_salary'])
            self.df = self.df[self.df['avg_salary'] > 0]

            # Handle outliers (salaries beyond 3 std devs)
            mean_salary = self.df['avg_salary'].mean()
            std_salary = self.df['avg_salary'].std()
            self.df = self.df[
                (self.df['avg_salary'] >= mean_salary - 3 * std_salary) &
                (self.df['avg_salary'] <= mean_salary + 3 * std_salary)
            ]

            log(f"After cleaning: {len(self.df)} rows")
            log(f"Salary range: ${self.df['avg_salary'].min():,.0f}K - ${self.df['avg_salary'].max():,.0f}K")
            log(f"Average salary: ${self.df['avg_salary'].mean():,.0f}K")

            self.dataset_loaded = True
            return True

        except Exception as e:
            log(f"Error loading dataset: {e}")
            self.dataset_loaded = False
            return False

    def train_model(self) -> bool:
        """Train enhanced ML model with feature engineering"""
        if not self.dataset_loaded or self.df is None:
            log("Dataset not loaded. Cannot train model.")
            return False

        try:
            log("Training enhanced ML model with feature engineering...")

            df_model = self.df.copy()

            # Feature Engineering

            # 1. Encode job title (keeping top 50, group others as 'other')
            if 'job_title' in df_model.columns:
                top_titles = df_model['job_title'].value_counts().head(50).index
                df_model['job_title_grouped'] = df_model['job_title'].apply(
                    lambda x: x if x in top_titles else 'other'
                )
                self.encoders['job_title'] = LabelEncoder()
                df_model['job_title_encoded'] = self.encoders['job_title'].fit_transform(
                    df_model['job_title_grouped'].astype(str)
                )

            # 2. Encode location/state
            if 'job_state' in df_model.columns:
                self.encoders['job_state'] = LabelEncoder()
                df_model['job_state_encoded'] = self.encoders['job_state'].fit_transform(
                    df_model['job_state'].astype(str)
                )
            elif 'location' in df_model.columns:
                # Extract state from location
                df_model['location_state'] = df_model['location'].str.split(',').str[-1].str.strip()
                self.encoders['location'] = LabelEncoder()
                df_model['location_encoded'] = self.encoders['location'].fit_transform(
                    df_model['location_state'].astype(str)
                )

            # 3. Company age (if available)
            if 'age' in df_model.columns:
                df_model['company_age'] = df_model['age']
            elif 'founded' in df_model.columns:
                current_year = 2024
                df_model['company_age'] = current_year - df_model['founded']
                df_model['company_age'] = df_model['company_age'].clip(0, 150)  # Cap at 150 years
            else:
                df_model['company_age'] = 10  # Default

            # 4. Skill features (binary flags)
            skill_cols = []
            for skill in ['python_yn', 'r_yn', 'spark', 'aws', 'excel']:
                if skill in df_model.columns:
                    df_model[skill] = df_model[skill].fillna(0)
                    skill_cols.append(skill)

            # 5. Skill count (total number of skills required)
            if skill_cols:
                df_model['skill_count'] = df_model[skill_cols].sum(axis=1)
            else:
                df_model['skill_count'] = 0

            # 6. Location cost of living (derived feature)
            if 'job_state' in df_model.columns:
                df_model['cost_of_living_index'] = df_model['job_state'].apply(
                    lambda x: self.col_index.get(x.lower() if isinstance(x, str) else '', 1.0)
                )
            else:
                df_model['cost_of_living_index'] = 1.0

            # 7. Rating (if available)
            if 'rating' in df_model.columns:
                df_model['company_rating'] = df_model['rating'].fillna(3.5)
            else:
                df_model['company_rating'] = 3.5

            # 8. Same state flag (if available)
            if 'same_state' in df_model.columns:
                df_model['same_state'] = df_model['same_state'].fillna(0)

            # Build feature list
            self.feature_columns = []

            # Categorical encoded features
            if 'job_title_encoded' in df_model.columns:
                self.feature_columns.append('job_title_encoded')
            if 'job_state_encoded' in df_model.columns:
                self.feature_columns.append('job_state_encoded')
            elif 'location_encoded' in df_model.columns:
                self.feature_columns.append('location_encoded')

            # Numerical features
            self.feature_columns.extend([
                'company_age',
                'skill_count',
                'cost_of_living_index',
                'company_rating'
            ])

            # Add skill binary features
            self.feature_columns.extend(skill_cols)

            if 'same_state' in df_model.columns:
                self.feature_columns.append('same_state')

            log(f"Using {len(self.feature_columns)} features: {self.feature_columns}")

            # Prepare train/test split
            X = df_model[self.feature_columns]
            y = df_model['avg_salary']

            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # Scale numerical features for better performance
            numerical_features = ['company_age', 'skill_count', 'cost_of_living_index', 'company_rating']
            numerical_idx = [self.feature_columns.index(f) for f in numerical_features if f in self.feature_columns]

            if numerical_idx:
                X_train_scaled = X_train.copy()
                X_test_scaled = X_test.copy()
                X_train_scaled.iloc[:, numerical_idx] = self.scaler.fit_transform(X_train.iloc[:, numerical_idx])
                X_test_scaled.iloc[:, numerical_idx] = self.scaler.transform(X_test.iloc[:, numerical_idx])
            else:
                X_train_scaled = X_train
                X_test_scaled = X_test

            # Train Gradient Boosting with optimized hyperparameters
            log("Training Gradient Boosting model with enhanced features...")
            self.model = GradientBoostingRegressor(
                n_estimators=200,        # More trees for better accuracy
                learning_rate=0.05,      # Lower learning rate with more trees
                max_depth=6,             # Deeper trees for complex patterns
                min_samples_split=10,    # Prevent overfitting
                min_samples_leaf=4,      # Prevent overfitting
                subsample=0.8,           # Use 80% of data for each tree (regularization)
                random_state=42,
                verbose=0
            )
            self.model.fit(X_train_scaled, y_train)

            # Evaluate model
            train_pred = self.model.predict(X_train_scaled)
            test_pred = self.model.predict(X_test_scaled)

            train_mae = mean_absolute_error(y_train, train_pred)
            test_mae = mean_absolute_error(y_test, test_pred)
            train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
            test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
            train_r2 = r2_score(y_train, train_pred)
            test_r2 = r2_score(y_test, test_pred)

            log(f"Model trained successfully!")
            log(f"   Train MAE: ${train_mae:,.0f}K | RMSE: ${train_rmse:,.0f}K | R2: {train_r2:.3f}")
            log(f"   Test MAE: ${test_mae:,.0f}K | RMSE: ${test_rmse:,.0f}K | R2: {test_r2:.3f}")

            # Feature importance
            if hasattr(self.model, 'feature_importances_'):
                importance = pd.DataFrame({
                    'feature': self.feature_columns,
                    'importance': self.model.feature_importances_
                }).sort_values('importance', ascending=False)
                log(f"\nTop 5 Most Important Features:")
                for idx, row in importance.head(5).iterrows():
                    log(f"   {row['feature']}: {row['importance']:.3f}")

            # Cross-validation
            cv_scores = cross_val_score(self.model, X_train_scaled, y_train, cv=5,
                                       scoring='neg_mean_absolute_error')
            log(f"\n5-Fold CV MAE: ${-cv_scores.mean():,.0f}K (+/- ${cv_scores.std():,.0f}K)")

            # Save model
            self._save_model()

            return True

        except Exception as e:
            log(f"Error training model: {e}")
            import traceback
            log(traceback.format_exc())
            return False

    def _save_model(self):
        """Save trained model, encoders, and scaler"""
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)

        joblib.dump(self.model, self.model_path)
        joblib.dump({
            'encoders': self.encoders,
            'feature_columns': self.feature_columns,
            'scaler': self.scaler
        }, self.encoders_path)

        log(f"Enhanced model saved to: {self.model_path}")

    def _load_model(self):
        """Load pre-trained model, encoders, and scaler"""
        try:
            self.model = joblib.load(self.model_path)
            encoder_data = joblib.load(self.encoders_path)
            self.encoders = encoder_data['encoders']
            self.feature_columns = encoder_data['feature_columns']
            self.scaler = encoder_data.get('scaler', StandardScaler())
            log("Enhanced model loaded successfully")
            self.dataset_loaded = True
        except Exception as e:
            log(f"Error loading model: {e}")
            self.dataset_loaded = False

    def predict_with_ml(self, job_title: str, location: str, experience_years: int,
                       skills: List[str] = None) -> Dict:
        """Predict salary using trained ML model with enhanced features"""
        if not self.dataset_loaded or self.model is None:
            raise ValueError("Model not trained. Cannot make predictions.")

        try:
            # Prepare input features
            features = {}

            # 1. Job title encoding
            if 'job_title' in self.encoders:
                try:
                    # Group rare titles as 'other'
                    if hasattr(self, 'df') and self.df is not None:
                        top_titles = self.df['job_title'].value_counts().head(50).index
                        job_title_grouped = job_title if job_title in top_titles else 'other'
                    else:
                        job_title_grouped = job_title

                    features['job_title_encoded'] = self.encoders['job_title'].transform([job_title_grouped])[0]
                except:
                    features['job_title_encoded'] = 0

            # 2. Location encoding (try state first, then location)
            if 'job_state' in self.encoders:
                try:
                    # Extract state from location string
                    state = location.split(',')[-1].strip()
                    features['job_state_encoded'] = self.encoders['job_state'].transform([state])[0]
                except:
                    features['job_state_encoded'] = 0
            elif 'location' in self.encoders:
                try:
                    state = location.split(',')[-1].strip()
                    features['location_encoded'] = self.encoders['location'].transform([state])[0]
                except:
                    features['location_encoded'] = 0

            # 3. Company age (use experience as proxy)
            features['company_age'] = max(experience_years + 5, 10)  # Assume company is older than candidate

            # 4. Skills features
            skills_lower = [s.lower() for s in (skills or [])]
            features['python_yn'] = 1 if 'python' in skills_lower else 0
            features['r_yn'] = 1 if 'r' in skills_lower else 0
            features['spark'] = 1 if 'spark' in skills_lower or 'pyspark' in skills_lower else 0
            features['aws'] = 1 if 'aws' in skills_lower or 'amazon web services' in skills_lower else 0
            features['excel'] = 1 if 'excel' in skills_lower or 'microsoft excel' in skills_lower else 0

            # 5. Skill count
            features['skill_count'] = sum([
                features['python_yn'], features['r_yn'],
                features['spark'], features['aws'], features['excel']
            ])

            # 6. Cost of living index
            city_state = location.lower().split(',')
            city = city_state[0].strip() if city_state else location.lower()
            state = city_state[-1].strip() if len(city_state) > 1 else ''

            col = self.col_index.get(city, self.col_index.get(state, 1.0))
            features['cost_of_living_index'] = col

            # 7. Company rating (default to good rating)
            features['company_rating'] = 3.8

            # 8. Same state (default to yes)
            if 'same_state' in self.feature_columns:
                features['same_state'] = 1

            # Create feature vector in correct order
            X = []
            for col in self.feature_columns:
                X.append(features.get(col, 0))

            X = np.array([X])

            # Scale numerical features
            numerical_features = ['company_age', 'skill_count', 'cost_of_living_index', 'company_rating']
            numerical_idx = [self.feature_columns.index(f) for f in numerical_features if f in self.feature_columns]

            if numerical_idx and hasattr(self.scaler, 'mean_'):
                X_scaled = X.copy()
                X_scaled[:, numerical_idx] = self.scaler.transform(X[:, numerical_idx])
            else:
                X_scaled = X

            # Predict
            predicted_salary = self.model.predict(X_scaled)[0]

            # Adjust for inflation and market growth (2018 -> 2024)
            # Tech salaries have grown ~30-40% since 2018 (6 years @ ~5-6% per year)
            # Dataset is from 2016-2018, so we need to adjust to current market
            inflation_multiplier = 1.35  # 35% increase over 6 years

            # Apply inflation adjustment
            predicted_salary = predicted_salary * inflation_multiplier

            # Apply skill premium (high-demand skills command higher salaries now)
            skill_premium = 1.0
            if features.get('python_yn', 0) == 1:
                skill_premium += 0.10  # Python is in very high demand
            if features.get('aws', 0) == 1:
                skill_premium += 0.12  # Cloud skills are premium
            if features.get('spark', 0) == 1:
                skill_premium += 0.08  # Big data skills valuable

            predicted_salary = predicted_salary * min(skill_premium, 1.3)  # Cap at 30% premium

            # Apply experience multiplier (more experience = exponentially higher pay)
            if experience_years >= 10:
                predicted_salary *= 1.25  # Senior multiplier
            elif experience_years >= 7:
                predicted_salary *= 1.15  # Mid-senior multiplier
            elif experience_years <= 2:
                predicted_salary *= 0.90  # Junior reduction

            # Ensure prediction is reasonable (between 40K and 500K)
            predicted_salary = np.clip(predicted_salary, 40, 500)

            # Calculate range (Â±12% for confidence interval)
            salary_min = int(predicted_salary * 0.88 * 1000)
            salary_max = int(predicted_salary * 1.12 * 1000)

            # Calculate percentiles (from training data distribution if available)
            if self.df is not None:
                similar_salaries = self.df['avg_salary'].values
                percentile_50 = int(np.percentile(similar_salaries, 50) * 1000)
                percentile_75 = int(np.percentile(similar_salaries, 75) * 1000)
                percentile_90 = int(np.percentile(similar_salaries, 90) * 1000)
            else:
                percentile_50 = int(predicted_salary * 1000)
                percentile_75 = int(predicted_salary * 1.15 * 1000)
                percentile_90 = int(predicted_salary * 1.30 * 1000)

            return {
                'min': salary_min,
                'max': salary_max,
                'average': int(predicted_salary * 1000),
                'median': percentile_50,
                'percentile_75': percentile_75,
                'percentile_90': percentile_90,
            }

        except Exception as e:
            log(f"ML prediction error: {e}")
            import traceback
            log(traceback.format_exc())
            raise

    def predict(self, job_title: str, location: str, experience_years: int = 3,
                skills: List[str] = None) -> Dict:
        """
        Main prediction function
        Uses enhanced ML model if available, otherwise falls back to mock data
        """
        try:
            if self.dataset_loaded and self.model is not None:
                # Use enhanced ML model
                log(f"Using enhanced ML model for prediction")
                ml_result = self.predict_with_ml(job_title, location, experience_years, skills)

                # Get COL adjustment
                col = self.get_cost_of_living(location)

                # Generate sources
                sources = [
                    {
                        'source': 'kaggle_dataset',
                        'min': ml_result['min'],
                        'max': ml_result['max'],
                        'average': ml_result['average'],
                        'url': 'Trained on Glassdoor salary data (742 records)',
                    },
                    {
                        'source': 'enhanced_ml_model',
                        'min': int(ml_result['min'] * 0.95),
                        'max': int(ml_result['max'] * 1.05),
                        'average': ml_result['average'],
                        'url': f'Gradient Boosting with {len(self.feature_columns)} features',
                    }
                ]

                recommendations = self._generate_recommendations(
                    job_title, location, experience_years, ml_result['average'], skills, True
                )

                return {
                    'jobTitle': job_title,
                    'location': location,
                    'experienceYears': experience_years,
                    'overallAverage': ml_result['average'],
                    'overallMin': ml_result['min'],
                    'overallMax': ml_result['max'],
                    'median': ml_result['median'],
                    'percentile75': ml_result['percentile_75'],
                    'percentile90': ml_result['percentile_90'],
                    'sources': sources,
                    'costOfLivingIndex': int(col * 100),
                    'recommendations': recommendations,
                    'modelUsed': 'enhanced_ml',
                }
            else:
                # Fallback to basic prediction
                log(f"Using fallback prediction (no trained model)")
                return self._fallback_prediction(job_title, location, experience_years, skills)

        except Exception as e:
            log(f"Prediction error: {e}")
            return self._fallback_prediction(job_title, location, experience_years, skills)

    def _fallback_prediction(self, job_title: str, location: str,
                            experience_years: int, skills: List[str]) -> Dict:
        """Fallback prediction using simple heuristics"""
        from salary_predictor import SalaryPredictor
        fallback = SalaryPredictor()
        result = fallback.predict(job_title, location, experience_years, skills)
        result['modelUsed'] = 'fallback_heuristic'
        return result

    def get_cost_of_living(self, location: str) -> float:
        """Get cost of living index"""
        city_state = location.lower().split(',')
        city = city_state[0].strip() if city_state else location.lower()
        state = city_state[-1].strip() if len(city_state) > 1 else ''
        return self.col_index.get(city, self.col_index.get(state, 1.0))

    def _generate_recommendations(self, job_title: str, location: str,
                                  experience_years: int, average_salary: int,
                                  skills: Optional[List[str]], is_ml: bool) -> List[str]:
        """Generate recommendations"""
        recommendations = []

        if is_ml:
            recommendations.append(
                f"Enhanced ML prediction using {len(self.feature_columns)} features including skills, location, and company data."
            )

        # Experience-based recommendations
        if experience_years < 3:
            recommendations.append(
                f"Entry-level: ${average_salary:,}. With 2-3 years experience, expect 25-35% increase."
            )
        elif experience_years >= 10:
            recommendations.append(
                f"Senior-level: ${average_salary:,}. Consider total compensation including equity and bonuses."
            )
        else:
            recommendations.append(
                f"Mid-level: ${average_salary:,}. Strong performance can yield 10-15% annual growth."
            )

        # Location recommendations
        col = self.get_cost_of_living(location)
        if col > 1.3:
            recommendations.append(
                f"High COL area (+{int((col-1)*100)}% adjustment). Remote positions may offer better value."
            )
        elif col < 0.95:
            recommendations.append(
                f"Lower COL area ({int((1-col)*100)}% below national average). Consider total quality of life."
            )

        # Skill-based recommendations
        if skills:
            high_value_skills = ['Python', 'AWS', 'Spark', 'Machine Learning', 'Deep Learning']
            has_high_value = any(s in str(skills) for s in high_value_skills)
            if has_high_value:
                recommendations.append(
                    f"High-value skills detected (Python/AWS/Spark/ML). You're in top tier - negotiate confidently."
                )

        recommendations.append(
            f"Negotiate within 10-20% of predicted salary. Research company-specific data for best results."
        )

        return recommendations


if __name__ == '__main__':
    if len(sys.argv) > 1:
        # API mode
        input_data = json.loads(sys.argv[1])
        predictor = EnhancedSalaryPredictor()
        result = predictor.predict(
            input_data['jobTitle'],
            input_data['location'],
            input_data.get('experienceYears', 3),
            input_data.get('skills', [])
        )
        print(json.dumps(result))
    else:
        # Test mode
        predictor = EnhancedSalaryPredictor()

        log("\nTest Predictions:")

        # Test 1: Junior Data Scientist
        log("\n1. Junior Data Scientist (2 years, Python, Excel)")
        result1 = predictor.predict('Data Scientist', 'San Francisco, CA', 2, ['Python', 'Excel'])
        log(f"   Predicted: ${result1['overallAverage']:,}")

        # Test 2: Senior Data Scientist
        log("\n2. Senior Data Scientist (8 years, Python, Spark, AWS)")
        result2 = predictor.predict('Data Scientist', 'San Francisco, CA', 8, ['Python', 'Spark', 'AWS'])
        log(f"   Predicted: ${result2['overallAverage']:,}")

        # Test 3: Mid-level in lower COL area
        log("\n3. Mid-level Data Scientist (5 years, Python, R)")
        result3 = predictor.predict('Data Scientist', 'Atlanta, GA', 5, ['Python', 'R'])
        log(f"   Predicted: ${result3['overallAverage']:,}")

        # Test 4: Different job title
        log("\n4. Machine Learning Engineer (6 years, Python, AWS, Spark)")
        result4 = predictor.predict('Machine Learning Engineer', 'Seattle, WA', 6, ['Python', 'AWS', 'Spark'])
        log(f"   Predicted: ${result4['overallAverage']:,}")
